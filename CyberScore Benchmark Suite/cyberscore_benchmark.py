"""
cyberscore_benchmark.py
Single-file system benchmark (Cyberpunk × Valorant style) using PyQt5.

Features:
- Single-file: UI, logic, styling, all inside this file.
- Tests: CPU, GPU (OpenGL if available else painter fallback), RAM, Disk, Network.
- Multithreaded tests using QThread; GUI on main thread.
- Neon styling, glowing panels, animated progress, glitch effects, Valorant-style final reveal.
- Clean, well-commented, class-based architecture.

Requirements:
- Python 3.7+
- PyQt5 (pip install PyQt5)
- Optional: PyOpenGL for OpenGL-based GPU test (pip install PyOpenGL)

Author: Generated by ChatGPT (GPT-5 Thinking mini)
"""

import sys
import os
import io
import time
import math
import hashlib
import tempfile
import threading
import random
import urllib.request
from functools import partial

# PyQt5 imports
from PyQt5.QtCore import (
    Qt, QThread, pyqtSignal, QTimer, QRectF, QPropertyAnimation, QEasingCurve, QPointF
)
from PyQt5.QtGui import (
    QColor, QFont, QPainter, QPen, QBrush, QPainterPath
)
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout, QPushButton,
    QLabel, QProgressBar, QFrame, QTextEdit, QSizePolicy, QStackedWidget
)

# Attempt to import OpenGL for a more intense GPU test. If not available, we fall back.
USE_PYOPENGL = False
try:
    from PyQt5.QtWidgets import QOpenGLWidget
    try:
        # Try to import PyOpenGL GL methods
        from OpenGL.GL import *
        USE_PYOPENGL = True
        HAVE_QOPENGL = True
    except Exception:
        # PyOpenGL not available; we can still use QOpenGLWidget and QPainter fallback
        HAVE_QOPENGL = True
        USE_PYOPENGL = False
except Exception:
    # No QOpenGLWidget available (older PyQt?). We'll do a painter fallback widget.
    HAVE_QOPENGL = False
    USE_PYOPENGL = False

# -----------------------------
# Helper functions & constants
# -----------------------------

NEON1 = "#00FFFF"  # cyan
NEON2 = "#FF2A6D"  # magenta-ish
NEON3 = "#08F7FE"  # bright blue
BG_COLOR = "#000000"  # black background
PANEL_ALPHA = "22"  # subtle transparency in hex (0x22)

# Safe conversion helper
def safe_div(a, b):
    try:
        return a / b
    except Exception:
        return float('inf')

# Simple smoothing for score transforms
def clamp(x, a, b):
    return max(a, min(b, x))

# -----------------------------
# Benchmark thread workers
# -----------------------------

# Base worker thread class with common signals
class BenchmarkThread(QThread):
    progress = pyqtSignal(int)           # 0-100 progress
    log = pyqtSignal(str)                # textual log lines
    finished_with_result = pyqtSignal(dict)  # emits dict of results

    def __init__(self, parent=None):
        super().__init__(parent)
        self._is_cancelled = False

    def cancel(self):
        self._is_cancelled = True

# CPU Benchmark: multi-threaded hashing loops
class CpuBenchmarkThread(BenchmarkThread):
    """
    Spawns multiple Python threads that compute SHA256 hashes repeatedly.
    Measures elapsed time for a fixed amount of work and reports throughput.
    """
    def __init__(self, threads=4, iterations_per_thread=200000, parent=None):
        super().__init__(parent)
        self.threads = max(1, threads)
        self.iterations = max(1000, iterations_per_thread)

    def run(self):
        self.log.emit(f"CPU Test: {self.threads} threads x {self.iterations} iterations")
        start = time.time()
        completed = 0
        lock = threading.Lock()

        def worker(seed):
            nonlocal completed
            data = (str(seed) * 32).encode('utf-8')
            # perform repeated hashing
            for i in range(self.iterations):
                if self._is_cancelled:
                    return
                hashlib.sha256(data + i.to_bytes(4, 'little')).digest()
                if i % 1000 == 0:
                    with lock:
                        # update progress globally
                        completed += 1000
                        total_work = self.iterations * self.threads
                        pct = int((completed / total_work) * 100)
                        self.progress.emit(clamp(pct, 0, 100))
            # finish remainder accounted in completed
        # Launch threads
        threads = []
        for t in range(self.threads):
            th = threading.Thread(target=worker, args=(t,))
            th.start()
            threads.append(th)
        for th in threads:
            th.join()
        elapsed = (time.time() - start) * 1000.0  # ms
        total_hashes = self.iterations * self.threads
        hashes_per_sec = safe_div(total_hashes, elapsed/1000.0)
        self.progress.emit(100)
        result = {
            "elapsed_ms": elapsed,
            "hashes": total_hashes,
            "hashes_per_sec": hashes_per_sec
        }
        self.log.emit(f"CPU: {total_hashes:,} hashes in {elapsed:.0f} ms ({hashes_per_sec:.0f} H/s)")
        self.finished_with_result.emit(result)

# RAM Benchmark: allocate large array (uses numpy if present; fallback to list)
class RamBenchmarkThread(BenchmarkThread):
    """
    Allocates and writes to a large buffer to test memory bandwidth.
    Uses numpy if available for realistic bandwidth; falls back to Python list operations otherwise.
    """
    def __init__(self, size_mb=500, parent=None):
        super().__init__(parent)
        self.size_mb = max(10, size_mb)

    def run(self):
        self.log.emit(f"RAM Test: Allocating ~{self.size_mb} MB")
        has_numpy = False
        try:
            import numpy as np
            has_numpy = True
        except Exception:
            has_numpy = False
        start = time.time()
        bytes_total = self.size_mb * 1024 * 1024
        # Write pass
        if has_numpy:
            import numpy as np
            # use float32 array
            count = bytes_total // 4
            self.log.emit("RAM: Using numpy float32 array")
            arr = np.zeros(count, dtype=np.float32)
            step = max(1, count // 100)
            for i in range(0, count, step):
                if self._is_cancelled:
                    return
                arr[i:i+step] = np.arange(step, dtype=np.float32) + (i % 100)
                pct = int((i / count) * 50)
                self.progress.emit(pct)
            # read pass
            s = 0.0
            for i in range(0, count, step):
                if self._is_cancelled:
                    return
                s += float(arr[i])
                pct = 50 + int((i / count) * 50)
                self.progress.emit(pct)
            # touch variable so arr isn't optimized away
            _ = s
        else:
            # pure python fallback: create a list of integers
            count = bytes_total // 8
            self.log.emit("RAM: numpy not found — using Python list fallback (slower)")
            arr = [0] * count
            step = max(1, count // 100)
            for i in range(0, count, step):
                if self._is_cancelled:
                    return
                for j in range(i, min(i+step, count)):
                    arr[j] = (j ^ i) & 0xFFFFFFFF
                pct = int((i / count) * 50)
                self.progress.emit(pct)
            s = 0
            for i in range(0, count, step):
                if self._is_cancelled:
                    return
                for j in range(i, min(i+step, count)):
                    s += arr[j]
                pct = 50 + int((i / count) * 50)
                self.progress.emit(pct)
            _ = s
        elapsed = (time.time() - start) * 1000.0
        mb_per_s = safe_div(self.size_mb * 2, elapsed/1000.0)  # read+write
        self.progress.emit(100)
        result = {
            "elapsed_ms": elapsed,
            "size_mb": self.size_mb,
            "bw_mb_s": mb_per_s
        }
        self.log.emit(f"RAM: {self.size_mb} MB roundtrip in {elapsed:.0f} ms ({mb_per_s:.1f} MB/s)")
        self.finished_with_result.emit(result)

# Disk Benchmark: write & read temp file
class DiskBenchmarkThread(BenchmarkThread):
    """
    Writes a temporary file of a specified size and reads it back to measure disk write/read speed.
    """
    def __init__(self, size_mb=100, parent=None):
        super().__init__(parent)
        self.size_mb = max(10, size_mb)

    def run(self):
        self.log.emit(f"Disk Test: Writing {self.size_mb} MB temp file")
        fname = os.path.join(tempfile.gettempdir(), f"cyberscore_temp_{int(time.time())}.bin")
        block = os.urandom(1024 * 1024)  # 1MB block
        total = self.size_mb
        # Write
        start_w = time.time()
        with open(fname, "wb") as f:
            for i in range(total):
                if self._is_cancelled:
                    try:
                        f.close()
                    except:
                        pass
                    if os.path.exists(fname):
                        os.remove(fname)
                    return
                f.write(block)
                pct = int((i / total) * 50)
                self.progress.emit(pct)
        elapsed_w = (time.time() - start_w)
        write_mb_s = safe_div(self.size_mb, elapsed_w) if elapsed_w > 0 else float('inf')
        self.log.emit(f"Disk: Wrote {self.size_mb} MB in {elapsed_w:.2f} s ({write_mb_s:.2f} MB/s)")
        # Read
        start_r = time.time()
        read_total = 0
        with open(fname, "rb") as f:
            while True:
                if self._is_cancelled:
                    return
                chunk = f.read(1024 * 1024)
                if not chunk:
                    break
                read_total += 1
                pct = 50 + int((read_total / total) * 50)
                self.progress.emit(pct)
        elapsed_r = (time.time() - start_r)
        read_mb_s = safe_div(self.size_mb, elapsed_r) if elapsed_r > 0 else float('inf')
        # Cleanup
        try:
            os.remove(fname)
        except Exception:
            pass
        self.progress.emit(100)
        result = {
            "write_s": elapsed_w,
            "write_mb_s": write_mb_s,
            "read_s": elapsed_r,
            "read_mb_s": read_mb_s,
            "size_mb": self.size_mb
        }
        self.log.emit(f"Disk: Read {self.size_mb} MB in {elapsed_r:.2f} s ({read_mb_s:.2f} MB/s)")
        self.finished_with_result.emit(result)

# Network Benchmark: repeated GET requests / download fixed size file
class NetworkBenchmarkThread(BenchmarkThread):
    """
    Measures latency and throughput by downloading a known URL or performing repeated GETs.
    Uses stdlib urllib to avoid extra deps.
    """
    def __init__(self, url=None, iterations=4, parent=None):
        super().__init__(parent)
        # default small test file; user can change to something closer to them
        self.url = url or "http://ipv4.download.thinkbroadband.com/5MB.zip"
        self.iterations = max(1, iterations)

    def run(self):
        self.log.emit(f"Network Test: Download {self.url} x {self.iterations}")
        total_bytes = 0
        latencies = []
        start_total = time.time()
        for i in range(self.iterations):
            if self._is_cancelled:
                return
            try:
                t0 = time.time()
                with urllib.request.urlopen(self.url, timeout=20) as r:
                    data = r.read()  # read entire file into memory
                t1 = time.time()
                elapsed = t1 - t0
                size = len(data)
                total_bytes += size
                latencies.append(elapsed)
                self.log.emit(f"Network: Iter {i+1} = {size/1024/1024:.2f} MB in {elapsed:.2f} s")
            except Exception as e:
                self.log.emit(f"Network: error on iteration {i+1}: {e}")
                latencies.append(float('inf'))
            pct = int(((i+1)/self.iterations) * 100)
            self.progress.emit(pct)
        elapsed_total = (time.time() - start_total)
        avg_latency = sum([l for l in latencies if math.isfinite(l)]) / max(1, len([l for l in latencies if math.isfinite(l)]))
        mb_s = safe_div(total_bytes / (1024*1024), elapsed_total) if elapsed_total > 0 else 0
        self.log.emit(f"Network: avg latency {avg_latency:.2f}s, throughput {mb_s:.2f} MB/s")
        result = {
            "avg_latency_s": avg_latency,
            "throughput_mb_s": mb_s,
            "total_mb": total_bytes / (1024*1024),
            "iterations": self.iterations
        }
        self.finished_with_result.emit(result)

# GPU Benchmark: QOpenGLWidget-based or Painter fallback
if HAVE_QOPENGL and USE_PYOPENGL:
    # Using PyOpenGL QOpenGLWidget rendering many triangles
    from PyQt5.QtWidgets import QOpenGLWidget
    class GpuOpenGLWidget(QOpenGLWidget):
        def __init__(self, parent=None):
            super().__init__(parent)
            self.timer = QTimer(self)
            self.timer.timeout.connect(self.update)
            self.timer.start(0)  # run as fast as possible
            self.last_time = time.time()
            self.frame_count = 0
            self.fps = 0.0
            self.stress = 1000  # number of shapes
        def initializeGL(self):
            glClearColor(0.0, 0.0, 0.0, 1.0)
            glEnable(GL_BLEND)
            glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)
        def paintGL(self):
            glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
            w, h = self.width(), self.height()
            glViewport(0, 0, w, h)
            # draw many colored quads/triangles
            glBegin(GL_TRIANGLES)
            for i in range(self.stress):
                if i % 3 == 0:
                    # random-ish moving positions to stress GPU
                    x = math.sin(time.time() * (0.1 + (i%7)/10.0) + i) * 0.8
                    y = math.cos(time.time() * (0.1 + (i%13)/10.0) + i) * 0.8
                    size = 0.005 + (i % 10) * 0.0005
                    col = ((i % 11) / 11.0, (i % 7) / 7.0, (i % 5) / 5.0, 0.8)
                    glColor4f(*col)
                    glVertex2f(x, y)
                    glVertex2f(x + size, y)
                    glVertex2f(x, y + size)
            glEnd()
            # fps calculation
            self.frame_count += 1
            now = time.time()
            if now - self.last_time >= 0.5:
                self.fps = self.frame_count / (now - self.last_time)
                self.frame_count = 0
                self.last_time = now

    class GpuBenchmarkThread(BenchmarkThread):
        """
        Launches a transient window with an OpenGL widget that renders as fast as possible for a short duration.
        Measures average fps.
        """
        def __init__(self, duration_s=4.0, parent=None):
            super().__init__(parent)
            self.duration = duration_s
            self._fps = 0.0

        def run(self):
            # Create an offscreen-like widget in the current Qt application thread. However, QOpenGLWidget must be created in main thread usually.
            # To keep this single-file and simple, we'll create and show a small hidden widget on the main thread via signal. But threads can't create widgets.
            # Simpler: instead of creating widget here, we will measure GPU in-place by asking the main GUI to run the widget and call back.
            # Therefore this thread will emit a log to indicate it's a placeholder; the main GUI will run the actual measurement.
            # We'll return quickly with placeholder.
            self.log.emit("GPU: OpenGL available — rendering test will be executed by UI (main thread).")
            # Signal that UI should perform GL test by sending an empty result asking UI to run actual test.
            result = {"ui_gl_probe": True, "duration_s": self.duration}
            self.finished_with_result.emit(result)

else:
    # Fallback painter-based spinner animation stress widget
    class GpuPainterWidget(QWidget):
        def __init__(self, parent=None):
            super().__init__(parent)
            self.timer = QTimer(self)
            self.timer.timeout.connect(self.on_tick)
            self.timer.start(16)  # ~60 FPS
            self.phase = 0.0
            self.frame_count = 0
            self.last_time = time.time()
            self.fps = 0.0
            self.stress = 500

        def on_tick(self):
            self.phase += 0.06
            self.update()
            self.frame_count += 1
            now = time.time()
            if now - self.last_time >= 0.5:
                self.fps = self.frame_count / (now - self.last_time)
                self.frame_count = 0
                self.last_time = now

        def paintEvent(self, event):
            p = QPainter(self)
            p.setRenderHint(QPainter.Antialiasing)
            w, h = self.width(), self.height()
            p.fillRect(0, 0, w, h, QColor(0,0,0))
            # draw many translucent rotating rectangles/ellipses to stress GPU
            for i in range(self.stress):
                t = (i / self.stress) * math.pi * 2
                x = int((w/2) + math.cos(self.phase + t) * (w/3) * ((i % 10)/10.0 + 0.2))
                y = int((h/2) + math.sin(self.phase + t) * (h/3) * ((i % 7)/7.0 + 0.2))
                size = max(1, int((i % 20) + 2))
                alpha = int(40 + (i % 60))
                color = QColor(int(0 + (i%2)*8), int(255 - (i%7)*12), int(200 + (i%3)*10), alpha)
                p.setPen(Qt.NoPen)
                p.setBrush(color)
                p.drawEllipse(x, y, size, size)
            # Draw fps indicator
            p.setPen(QColor(255,255,255))
            p.setFont(QFont("Arial", 10))
            p.drawText(10, 20, f"FPS: {self.fps:.1f}")

    class GpuBenchmarkThread(BenchmarkThread):
        """
        Tells the main UI to run the painter-based widget for a short duration and returns the measured fps.
        This thread is a placeholder; the UI will coordinate actual rendering measurement.
        """
        def __init__(self, duration_s=4.0, parent=None):
            super().__init__(parent)
            self.duration = duration_s

        def run(self):
            self.log.emit("GPU: PyOpenGL not available — using painter fallback for GPU test.")
            result = {"ui_painter_probe": True, "duration_s": self.duration}
            self.finished_with_result.emit(result)

# -----------------------------
# Main Application / GUI
# -----------------------------
class CyberScoreApp(QMainWindow):
    """
    Main application window. Single class-based architecture as required.
    All neon styling is defined in Qt stylesheets within this file.
    """
    def __init__(self):
        super().__init__()
        self.setWindowTitle("CyberScore Benchmark")
        self.setMinimumSize(1000, 700)
        # Central widget and layout
        central = QWidget()
        self.setCentralWidget(central)
        main_layout = QVBoxLayout()
        central.setLayout(main_layout)
        self.setStyleSheet(self._build_stylesheet())
        # Header
        header = QLabel("CYBERSCORE  •  Benchmark Suite")
        header.setObjectName("headerLabel")
        header.setAlignment(Qt.AlignCenter)
        main_layout.addWidget(header)
        # Main panels layout
        panels = QHBoxLayout()
        main_layout.addLayout(panels, stretch=1)
        # Left: Controls & logs
        left_panel = QVBoxLayout()
        panels.addLayout(left_panel, stretch=3)
        # Buttons
        self.start_button = QPushButton("RUN FULL BENCHMARK")
        self.start_button.setObjectName("neonButton")
        self.start_button.clicked.connect(self.on_run_benchmark)
        left_panel.addWidget(self.start_button)
        self.abort_button = QPushButton("ABORT")
        self.abort_button.setObjectName("abortButton")
        self.abort_button.clicked.connect(self.on_abort)
        self.abort_button.setEnabled(False)
        left_panel.addWidget(self.abort_button)
        # Progress bars for each test
        self.progress_widgets = {}
        for name in ["CPU", "GPU", "RAM", "Disk", "Network"]:
            lbl = QLabel(name)
            lbl.setObjectName("panelTitle")
            left_panel.addWidget(lbl)
            pb = QProgressBar()
            pb.setRange(0, 100)
            pb.setValue(0)
            pb.setTextVisible(True)
            pb.setObjectName("neonProgress")
            left_panel.addWidget(pb)
            self.progress_widgets[name] = pb
        # Live logs text area
        self.log_view = QTextEdit()
        self.log_view.setReadOnly(True)
        self.log_view.setObjectName("logView")
        left_panel.addWidget(self.log_view, stretch=2)
        # Right: HUD / score reveal area
        right_panel = QVBoxLayout()
        panels.addLayout(right_panel, stretch=4)
        # HUD panel
        hud = QFrame()
        hud.setObjectName("hudFrame")
        hud_layout = QVBoxLayout()
        hud.setLayout(hud_layout)
        right_panel.addWidget(hud, stretch=1)
        # HUD: real-time metrics and GPU render area
        self.metrics_label = QLabel("Ready. Press RUN to start benchmarks.")
        self.metrics_label.setObjectName("metricsLabel")
        self.metrics_label.setAlignment(Qt.AlignCenter)
        hud_layout.addWidget(self.metrics_label)
        # GPU render preview (either OpenGL widget or painter fallback)
        if HAVE_QOPENGL and USE_PYOPENGL:
            # Create OpenGL preview
            self.gpu_preview = GpuOpenGLWidget()
            self.gpu_preview.setMinimumHeight(260)
            hud_layout.addWidget(self.gpu_preview)
        else:
            # Use painter-based widget
            self.gpu_preview = GpuPainterWidget()
            self.gpu_preview.setMinimumHeight(260)
            hud_layout.addWidget(self.gpu_preview)
        # Score stacked area: shows interim results then final reveal
        self.stack = QStackedWidget()
        right_panel.addWidget(self.stack, stretch=2)
        # Page 0: Intermediate results table / readout
        self.results_widget = QTextEdit()
        self.results_widget.setReadOnly(True)
        self.results_widget.setObjectName("resultsView")
        self.stack.addWidget(self.results_widget)
        # Page 1: Final big score reveal
        self.reveal_widget = QWidget()
        self.reveal_widget.setObjectName("revealWidget")
        rlay = QVBoxLayout()
        self.reveal_widget.setLayout(rlay)
        self.score_label = QLabel("0")
        self.score_label.setObjectName("scoreLabel")
        self.score_label.setAlignment(Qt.AlignCenter)
        self.rank_label = QLabel("")
        self.rank_label.setObjectName("rankLabel")
        self.rank_label.setAlignment(Qt.AlignCenter)
        rlay.addStretch()
        rlay.addWidget(self.score_label)
        rlay.addWidget(self.rank_label)
        rlay.addStretch()
        self.stack.addWidget(self.reveal_widget)
        # Glitch timer for subtle effects
        self.glitch_timer = QTimer(self)
        self.glitch_timer.timeout.connect(self._glitch_tick)
        self.glitch_timer.start(600)  # every 600ms glitch check
        self._glitch_state = False
        # Internal state
        self.current_workers = []
        self.partial_results = {}
        self._aborted = False
        # show window
        self.show()

    # -----------------------------
    # Styling: CSS for neon look
    # -----------------------------
    def _build_stylesheet(self):
        # All styles defined here as a single CSS string
        css = f"""
        QWidget {{
            background-color: {BG_COLOR};
            color: #E6F7FF;
            font-family: "Segoe UI", "Roboto", "Arial";
        }}
        #headerLabel {{
            font-size: 22px;
            padding: 14px;
            border: 2px solid rgba(255,255,255,0.03);
            color: {NEON3};
            letter-spacing: 2px;
            text-shadow: 0 0 12px {NEON3};
        }}
        #neonButton, QPushButton#neonButton {{
            background: qlineargradient(x1:0, y1:0, x2:1, y2:1, stop:0 {NEON1}, stop:1 {NEON3});
            border-radius: 10px;
            padding: 12px;
            font-weight: bold;
            color: black;
            box-shadow: 0 0 20px {NEON1};
            border: 1px solid rgba(255,255,255,0.12);
        }}
        QPushButton#neonButton:hover {{
            transform: scale(1.02);
        }}
        QPushButton#abortButton {{
            background: qlineargradient(x1:0, y1:0, x2:1, y2:1, stop:0 {NEON2}, stop:1 {NEON1});
            border-radius: 8px;
            padding: 8px;
            font-weight: bold;
            color: white;
        }}
        #panelTitle {{
            color: {NEON1};
            font-weight: 700;
            margin-top: 8px;
            margin-bottom: 4px;
        }}
        QProgressBar#neonProgress {{
            height: 18px;
            border-radius: 9px;
            background: rgba(255,255,255,0.03);
            border: 1px solid rgba(255,255,255,0.04);
            text-align: center;
            color: white;
        }}
        QProgressBar#neonProgress::chunk {{
            border-radius: 8px;
            background: qlineargradient(x1:0, y1:0, x2:1, y2:0, stop:0 {NEON3}, stop:0.5 {NEON1}, stop:1 {NEON2});
            box-shadow: 0 0 12px {NEON1};
        }}
        #hudFrame {{
            border: 1px solid rgba(255,255,255,0.04);
            border-radius: 12px;
            padding: 8px;
            background: rgba(255,255,255,0.02);
            box-shadow: 0 0 40px rgba(0,255,255,0.04);
        }}
        #metricsLabel {{
            color: {NEON3};
            font-weight: 600;
            font-size: 14px;
            padding: 6px;
            text-align: center;
        }}
        QTextEdit#logView {{
            background: rgba(0,0,0,0.6);
            border-radius: 8px;
            border: 1px solid rgba(255,255,255,0.03);
            padding: 8px;
            font-family: "Consolas", monospace;
            font-size: 12px;
            color: #BFEFFF;
        }}
        QTextEdit#resultsView {{
            background: rgba(0,0,0,0.6);
            border-radius: 8px;
            border: 1px solid rgba(255,255,255,0.03);
            padding: 8px;
            font-family: "Consolas", monospace;
            font-size: 13px;
            color: #CFF9FF;
        }}
        #revealWidget {{
            background: transparent;
        }}
        QLabel#scoreLabel {{
            font-size: 84px;
            color: {NEON2};
            text-shadow: 0 0 30px {NEON2};
            font-weight: 900;
            letter-spacing: 6px;
        }}
        QLabel#rankLabel {{
            font-size: 20px;
            color: {NEON1};
            text-shadow: 0 0 12px {NEON1};
            font-weight: 700;
            margin-top: -8px;
        }}
        """
        return css

    # -----------------------------
    # UI Interactions and workers
    # -----------------------------
    def on_run_benchmark(self):
        # Disable the start button & enable abort
        self.start_button.setEnabled(False)
        self.abort_button.setEnabled(True)
        self._aborted = False
        self.clear_logs()
        self.partial_results = {}
        self.stack.setCurrentIndex(0)  # show results view
        self.results_widget.clear()
        self.metrics_label.setText("Running full benchmark...")
        # Reset progress bars
        for pb in self.progress_widgets.values():
            pb.setValue(0)
        # Launch threads in sequence or in parallel? We'll run CPU, RAM, Disk, Network sequentially,
        # but start a GPU probe via the UI for proper QOpenGLWidget usage (since GL widgets need main thread).
        # Using sequential simplifies computing overall progress and reduces I/O contention.
        self.jobs = [
            ("CPU", CpuBenchmarkThread(threads=max(2, os.cpu_count()//1), iterations_per_thread=40000)),
            ("GPU", GpuBenchmarkThread(duration_s=4.0)),
            ("RAM", RamBenchmarkThread(size_mb=300)),
            ("Disk", DiskBenchmarkThread(size_mb=100)),
            ("Network", NetworkBenchmarkThread(iterations=2))
        ]
        # Kick off the first job
        self.job_index = 0
        self._start_next_job()

    def _start_next_job(self):
        if self._aborted:
            self._finish_all(aborted=True)
            return
        if self.job_index >= len(self.jobs):
            self._compute_final_score_and_reveal()
            return
        name, worker = self.jobs[self.job_index]
        self.log(f"Starting {name} test...")
        # Connect signals
        worker.progress.connect(partial(self._on_progress, name))
        worker.log.connect(self.log)
        worker.finished_with_result.connect(partial(self._on_job_finished, name))
        # Keep reference so we can cancel
        self.current_workers.append(worker)
        # Start worker thread
        worker.start()
        # If GPU worker returns a ui_probe result, the finished signal will instruct UI to run GL probe.
        # The worker thread itself simply emits that request and ends.

    def _on_progress(self, test_name, pct):
        # Set associated progress bar
        try:
            self.progress_widgets[test_name].setValue(int(pct))
        except Exception:
            pass

    def _on_job_finished(self, test_name, result):
        # Called in main thread because signal emitted there
        self.log(f"{test_name} finished.")
        # Store partial result
        self.partial_results[test_name] = result
        # Special handling for GPU "ui_gl_probe" or "ui_painter_probe"
        if isinstance(result, dict) and ("ui_gl_probe" in result or "ui_painter_probe" in result):
            # launch UI-side GPU measurement for the duration requested
            dur = result.get("duration_s", 4.0)
            self._run_ui_gpu_probe(duration=dur)
            # After UI probe, continue to next job (the UI probe will call _on_ui_gpu_done)
            return
        # Move to next job
        self.job_index += 1
        self._start_next_job()

    def _run_ui_gpu_probe(self, duration=4.0):
        # Perform GPU rendering in the UI main thread by measuring fps on the preview widget for `duration` seconds.
        self.log("GPU: Performing UI-render probe...")
        preview = self.gpu_preview
        # We'll read fps measured inside the widget for the given duration.
        # Show a countdown on metrics label
        end_time = time.time() + duration
        self._gpu_fps_samples = []
        def sample():
            # Called periodically in main thread
            fps = getattr(preview, 'fps', 0.0)
            if math.isfinite(fps) and fps > 0:
                self._gpu_fps_samples.append(fps)
            remaining = int(end_time - time.time())
            self.metrics_label.setText(f"GPU testing... {max(0, remaining)}s left — last FPS: {fps:.1f}")
            if time.time() >= end_time:
                # stop timer and record result
                sampler.stop()
                avg_fps = sum(self._gpu_fps_samples) / max(1, len(self._gpu_fps_samples))
                self.log(f"GPU: Average FPS = {avg_fps:.1f}")
                self.progress_widgets["GPU"].setValue(100)
                # store result and continue
                self.partial_results["GPU"] = {"avg_fps": avg_fps}
                # continue with next job
                self.job_index += 1
                self._start_next_job()
        # Use a QTimer-based sampler
        sampler = QTimer(self)
        sampler.setInterval(200)
        sampler.timeout.connect(sample)
        sampler.start()

    def on_abort(self):
        self._aborted = True
        self.log("Abort requested — stopping all running tests...")
        # Cancel running threads
        for w in self.current_workers:
            try:
                w.cancel()
            except Exception:
                pass
        self.abort_button.setEnabled(False)
        self.start_button.setEnabled(True)

    def _finish_all(self, aborted=False):
        if aborted:
            self.metrics_label.setText("Benchmark aborted.")
        else:
            self.metrics_label.setText("All tests completed.")
        self.start_button.setEnabled(True)
        self.abort_button.setEnabled(False)

    # -----------------------------
    # Final score computation & reveal
    # -----------------------------
    def _compute_final_score_and_reveal(self):
        # Create textual results
        txt = []
        txt.append("=== Raw Results ===\n")
        for k, v in self.partial_results.items():
            txt.append(f"[{k}] {v}\n")
        self.results_widget.setPlainText("\n".join(txt))
        # Heuristic scoring: convert each metric to 0-250 (so total max = 1250) then rescale to 0-1000
        # CPU: hashes_per_sec (higher better)
        cpu_score = 0.0
        if "CPU" in self.partial_results and isinstance(self.partial_results["CPU"], dict):
            cpu = self.partial_results["CPU"]
            hps = cpu.get("hashes_per_sec", 0.0)
            # baseline: 50k H/s = 50 -> scale
            cpu_score = clamp((math.log1p(hps) / math.log1p(1e6)) * 250.0, 0, 250)
        # GPU: avg_fps
        gpu_score = 0.0
        if "GPU" in self.partial_results:
            gpu = self.partial_results["GPU"]
            fps = gpu.get("avg_fps", 0.0) or gpu.get("avg_fps", 0.0)
            gpu_score = clamp((fps / 240.0) * 250.0, 0, 250)
        # RAM: bw_mb_s
        ram_score = 0.0
        if "RAM" in self.partial_results:
            ram = self.partial_results["RAM"]
            bw = ram.get("bw_mb_s", ram.get("bw_mb_s", 0.0))
            ram_score = clamp((bw / 20000.0) * 250.0, 0, 250)  # generous ceiling
        # Disk: combined read/write
        disk_score = 0.0
        if "Disk" in self.partial_results:
            d = self.partial_results["Disk"]
            w = d.get("write_mb_s", 0.0)
            r = d.get("read_mb_s", 0.0)
            disk_score = clamp(((w + r)/2.0 / 1000.0) * 250.0, 0, 250)
        # Network: throughput
        net_score = 0.0
        if "Network" in self.partial_results:
            n = self.partial_results["Network"]
            t = n.get("throughput_mb_s", 0.0)
            net_score = clamp((t / 100.0) * 250.0, 0, 250)
        # Aggregate (we may cap to 1000)
        raw_total = cpu_score + gpu_score + ram_score + disk_score + net_score
        # Normalize to 0-1000 (since max possible raw_total could be 1250 if all are near-ceiling)
        normalized = int(clamp((raw_total / 1250.0) * 1000.0, 0, 1000))
        self.cyberscore = normalized
        # Prepare reveal
        # Fill the reveal label with 0 and animate to final
        self.score_label.setText("0")
        self.rank_label.setText(self._rank_text(normalized))
        self.stack.setCurrentIndex(1)
        # Start number animation
        self._animate_score_reveal(target=normalized)

    def _rank_text(self, score):
        if score >= 900:
            return "Masterclass — System Dominator"
        elif score >= 750:
            return "Legend — Elite Performance"
        elif score >= 500:
            return "Pro — Strong"
        elif score >= 250:
            return "Competent — Average"
        else:
            return "Starter — Needs Upgrade"

    def _animate_score_reveal(self, target=0):
        # Smooth numeric update + neon flicker
        duration = 2000  # ms
        start = 0
        steps = max(20, duration // 30)
        def tick(i):
            val = int((i/steps) * target)
            self.score_label.setText(str(val))
            # small scaling animation
            self.score_label.setStyleSheet("transform: scale(1.0);")
        for i in range(steps+1):
            QTimer.singleShot(int(i*(duration/steps)), partial(tick, i))
        # final glow/pulse
        def final_pulse():
            self.score_label.setText(str(target))
            # tiny animated color changes using QPropertyAnimation on geometry/palette is complex;
            # instead we'll do simple repeated timer pulses for glow
            pulse_count = 6
            def pstep(j):
                if j % 2 == 0:
                    self.score_label.setStyleSheet("color: {0}; text-shadow: 0 0 30px {0};".format(NEON2))
                else:
                    self.score_label.setStyleSheet("color: {0}; text-shadow: 0 0 6px {0};".format(NEON3))
            for j in range(pulse_count):
                QTimer.singleShot(j*200, partial(pstep, j))
            # restore style after pulses
            QTimer.singleShot(pulse_count*200 + 100, lambda: self.score_label.setStyleSheet(""))
            # finish
            self.metrics_label.setText(f"CyberScore: {target}")
            self.start_button.setEnabled(True)
            self.abort_button.setEnabled(False)
        QTimer.singleShot(duration + 100, final_pulse)

    # -----------------------------
    # Logging & Glitch UI effects
    # -----------------------------
    def log(self, text):
        ts = time.strftime("%H:%M:%S")
        self.log_view.append(f"[{ts}] {text}")

    def clear_logs(self):
        self.log_view.clear()

    def _glitch_tick(self):
        # Subtle glitch: occasionally flicker progress bar text or distort header label slightly
        self._glitch_state = not self._glitch_state
        if random.random() < 0.15 and self._glitch_state:
            # flicker header text color
            header = self.findChild(QLabel, "headerLabel")
            if header:
                header.setStyleSheet("color: rgba(255,255,255,0.06);")
                QTimer.singleShot(120, lambda: header.setStyleSheet(""))
        # small textual distortion in metrics label
        if random.random() < 0.12:
            orig = self.metrics_label.text()
            distorted = self._distort_text(orig)
            self.metrics_label.setText(distorted)
            QTimer.singleShot(160, lambda: self.metrics_label.setText(orig))

    def _distort_text(self, t):
        # Insert zero-width spaces or swap chars for a subtle glitch
        if len(t) < 4:
            return t
        i = random.randint(0, len(t)-2)
        lst = list(t)
        lst[i], lst[i+1] = lst[i+1], lst[i]
        return "".join(lst)

# -----------------------------
# Main entrypoint
# -----------------------------
def main():
    app = QApplication(sys.argv)
    # create and show app
    win = CyberScoreApp()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main()
